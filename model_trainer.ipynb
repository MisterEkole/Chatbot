{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import numpy as np\r\n",
    "import random, json\r\n",
    "import nltk_utils\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "\r\n",
    "from nltk_utils import bag_words, tokeniser, stem\r\n",
    "import model\r\n",
    "from model import Network"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "with open('intents.json','r') as f:\r\n",
    "    intents=json.load(f)\r\n",
    "\r\n",
    "Words=[]\r\n",
    "tags=[]\r\n",
    "xy=[]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "for intent in intents['intents']:\r\n",
    "    tag=intent['tag']\r\n",
    "    tags.append(tag)\r\n",
    "    \r\n",
    "    for pattern in intent['patterns']:\r\n",
    "        w=tokeniser(pattern)\r\n",
    "        Words.extend(w)\r\n",
    "        xy.append((w,tag))\r\n",
    "words_to_ignore=['?',\".\",'!']\r\n",
    "Words=[stem(w) for w in Words if w not in words_to_ignore]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Sorting all words, removing duplicated\r\n",
    "Words=sorted(set(Words))\r\n",
    "tags= sorted(set(tags))\r\n",
    "\r\n",
    "print(len(xy), \"patterns\")\r\n",
    "\r\n",
    "print(len(tags), \"tags:\", tags)\r\n",
    "print(len(Words), \"Unique words\", Words)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "26 patterns\n",
      "7 tags: ['delivery', 'funny', 'goodbye', 'greeting', 'items', 'payments', 'thanks']\n",
      "54 Unique words [\"'s\", 'a', 'accept', 'anyon', 'are', 'bye', 'can', 'card', 'cash', 'credit', 'day', 'deliveri', 'do', 'doe', 'funni', 'get', 'good', 'goodby', 'have', 'hello', 'help', 'hey', 'hi', 'how', 'i', 'is', 'item', 'joke', 'kind', 'know', 'later', 'long', 'lot', 'mastercard', 'me', 'my', 'of', 'onli', 'pay', 'paypal', 'see', 'sell', 'ship', 'someth', 'take', 'tell', 'thank', 'that', 'there', 'what', 'when', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#Creating trainin dataset\r\n",
    "\r\n",
    "X_train=[]\r\n",
    "Y_train=[]\r\n",
    "\r\n",
    "\r\n",
    "for (pattern_sentence, tag) in xy:\r\n",
    "    #X: Bag of words for each pattern sentence\r\n",
    "    bag= bag_words(pattern_sentence, Words)\r\n",
    "    X_train.append(bag)\r\n",
    "    #Y: Class labels\r\n",
    "    \r\n",
    "    label=tags.index(tag)\r\n",
    "    Y_train.append(label)\r\n",
    "    \r\n",
    "X_train=np.array(X_train)\r\n",
    "Y_train=np.array(Y_train)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "#Specifying hyper parameters\r\n",
    "\r\n",
    "eporchs=1000\r\n",
    "batch_size=8\r\n",
    "lr=0.001\r\n",
    "inputs=len(X_train[0])\r\n",
    "hidden=8\r\n",
    "outputs= len(tags)\r\n",
    "\r\n",
    "print(inputs, outputs)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "54 7\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "#creating chatdat class to load dataset\r\n",
    "class ChataData(Dataset):\r\n",
    "    def __init__(self):\r\n",
    "        self.n_samples= len(X_train)\r\n",
    "        \r\n",
    "        self.x_data= X_train\r\n",
    "        self.y_data= Y_train\r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.x_data[index], self.y_data[index]\r\n",
    "    \r\n",
    "    def __len__(self):\r\n",
    "        return self.n_samples\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "chat_data=ChataData()\r\n",
    "#loading the trainloader\r\n",
    "train_loader= DataLoader(dataset=chat_data, batch_size=batch_size, shuffle=True, num_workers=0)\r\n",
    "\r\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "model= Network(inputs, hidden, outputs).to(device)\r\n",
    "criterion=nn.CrossEntropyLoss()\r\n",
    "optimiser=torch.optim.Adam(model.parameters(), lr=lr)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "#training the neural network\r\n",
    "for epoch in range(eporchs):\r\n",
    "    for(words, labels) in train_loader:\r\n",
    "        words= words.to(device)\r\n",
    "       \r\n",
    "        labels=labels.to(dtype=torch.long).to(device)\r\n",
    "        \r\n",
    "        #forward pass\r\n",
    "        outputs= model(words)\r\n",
    "        \r\n",
    "        loss=criterion(outputs, labels)\r\n",
    "        \r\n",
    "        #backprop and optimisation\r\n",
    "        \r\n",
    "        optimiser.zero_grad()\r\n",
    "        \r\n",
    "        loss.backward()\r\n",
    "        optimiser.step()\r\n",
    "        \r\n",
    "        if(epoch+1)%100==0:\r\n",
    "            print(f'Epoch [{epoch+1}/{eporchs}], Loss: {loss.item():.4f}')\r\n",
    "print(f'final loss: {loss.item():.4f}')\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [100/1000], Loss: 1.1471\n",
      "Epoch [100/1000], Loss: 1.1813\n",
      "Epoch [100/1000], Loss: 0.9249\n",
      "Epoch [100/1000], Loss: 1.0999\n",
      "Epoch [200/1000], Loss: 0.1015\n",
      "Epoch [200/1000], Loss: 0.0917\n",
      "Epoch [200/1000], Loss: 0.1836\n",
      "Epoch [200/1000], Loss: 0.2116\n",
      "Epoch [300/1000], Loss: 0.0161\n",
      "Epoch [300/1000], Loss: 0.0241\n",
      "Epoch [300/1000], Loss: 0.0333\n",
      "Epoch [300/1000], Loss: 0.0085\n",
      "Epoch [400/1000], Loss: 0.0071\n",
      "Epoch [400/1000], Loss: 0.0094\n",
      "Epoch [400/1000], Loss: 0.0070\n",
      "Epoch [400/1000], Loss: 0.0204\n",
      "Epoch [500/1000], Loss: 0.0031\n",
      "Epoch [500/1000], Loss: 0.0047\n",
      "Epoch [500/1000], Loss: 0.0060\n",
      "Epoch [500/1000], Loss: 0.0031\n",
      "Epoch [600/1000], Loss: 0.0031\n",
      "Epoch [600/1000], Loss: 0.0018\n",
      "Epoch [600/1000], Loss: 0.0026\n",
      "Epoch [600/1000], Loss: 0.0042\n",
      "Epoch [700/1000], Loss: 0.0019\n",
      "Epoch [700/1000], Loss: 0.0012\n",
      "Epoch [700/1000], Loss: 0.0021\n",
      "Epoch [700/1000], Loss: 0.0011\n",
      "Epoch [800/1000], Loss: 0.0007\n",
      "Epoch [800/1000], Loss: 0.0014\n",
      "Epoch [800/1000], Loss: 0.0011\n",
      "Epoch [800/1000], Loss: 0.0022\n",
      "Epoch [900/1000], Loss: 0.0012\n",
      "Epoch [900/1000], Loss: 0.0007\n",
      "Epoch [900/1000], Loss: 0.0005\n",
      "Epoch [900/1000], Loss: 0.0005\n",
      "Epoch [1000/1000], Loss: 0.0005\n",
      "Epoch [1000/1000], Loss: 0.0008\n",
      "Epoch [1000/1000], Loss: 0.0005\n",
      "Epoch [1000/1000], Loss: 0.0006\n",
      "final loss: 0.0006\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "#savind the trained model\r\n",
    "models={\r\n",
    "    \"model_state\": model.state_dict(),\r\n",
    "    \"inputs\": inputs,\r\n",
    "    \"hidden\": hidden,\r\n",
    "    \"outputs\": outputs,\r\n",
    "    \"tags\": tags,\r\n",
    "    \"Words\": Words\r\n",
    "}\r\n",
    "\r\n",
    "File=\"models.pth\"\r\n",
    "\r\n",
    "torch.save(models, File)\r\n",
    "\r\n",
    "print(f'training complete. Model saved to {File}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training complete. Model saved to models.pth\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit"
  },
  "interpreter": {
   "hash": "bee0532d505904fbb5a3281d4f74ffd15ae670e264452312d08c7f434d534dd4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}